## 13/11-18/11: 
### Objective: read, implement and understand all the provided documents and code
**Khanh:**
- Read and fully understand how RAG works in theory.
- Explore LLMs alternative for openAI.
- Familiarize with HuggingFace.
- Read documentation of PyTorch, TensorFlow, and Langchain.
- Rewrite Thomas Rocciaâ€™s code using SciBert and Bart.
## 20/11-25/11: 
### Objective:combine Catherine's method with TRAM, implement RAG with LLAMA-2 Model
**Khanh:**
- Using Catherine's asset matching and asset similarity matching in conjunction with TRAM (modified TRAM code)
- Implement PineCone using a database from Hugging face
- Set Up Jira for task allocations
- Implement RAG and Llama
- Manual Query and Prompt trials for dependencies
- Read sample attack flows in more details to understand what the LLM model could be used for
- Researched dependency matching and sentence encoding papers (uploaded to git)
## 27/11-2/12: 
### Objective:TRAM and Catherine testing, update data loading and prompt engineering on RAG.
**Khanh:**
- Created codebase to test TRAM automatically by examining a sentence one by one instead of feeding in a document
- Retrieved datasets for testing TRAM and Catherine
- Collect and Analyse the Results in Latex
- Examining inconsistencies and refining the testing methodologies
- Reread through Catherine's, TRAM's, Dessert Lab's testing methodologies
- Attempted to understand and utilise the ML server
- Incorporating TRAM and RAG
## 4/12-9/12: 
### Objective:TRAM testing on external data set, Continue with RAG
**Khanh:**
- Examine TRAM's single vs multi label model to see performance difference
- Examine why performance was so poor for some datasets (final solutions - cleaning datasets/string dropping some subtechniques)
- Worked on the code for RAG to analyse Tram's result and produce Attack Flow
- Using prompts to extract the potential assets to replace Catherine's method

**?:**
- Start looking into transforming csv result file into json, to later transform into afb
## 11/12-16/12:
